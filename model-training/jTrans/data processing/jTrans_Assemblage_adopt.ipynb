{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assemblage adoption code for jTrans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please put the file containing function ids you want to train on into a file, and give it as TXTINPUT i, dbfile as the Assemblage db file path, dataset_path as the binaries path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import os\n",
    "import sqlite3\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "\n",
    "def getmd5(s):\n",
    "    return hashlib.md5(s.encode()).hexdigest()\n",
    "\n",
    "dbfile = './sept25.sqlite'\n",
    "dataset_path = './dataset_sept25'\n",
    "flatten_dir = \"./dataset\"\n",
    "\n",
    "TXTINPUT = \"train_function_id.txt\"\n",
    "CSVOUT = TXTINPUT.replace('.txt', '.csv')\n",
    "\n",
    "# Cols function_id, function_name,rvas.start, binary_id, binary_compiler, binary_optimization, binary_github_url\n",
    "with open(TXTINPUT) as f:\n",
    "    fids = [int(x.strip()) for x in f.readlines()]\n",
    "    \n",
    "connection = sqlite3.connect(dbfile)\n",
    "cursor = connection.cursor()\n",
    "\n",
    "csvrows = []\n",
    "\n",
    "\n",
    "infos = cursor.execute('''SELECT f.id, f.name, r.start, b.id AS binary_id, b.toolset_version, b.optimization, b.github_url\n",
    "    FROM functions f\n",
    "    JOIN rvas r ON r.function_id = f.id\n",
    "    JOIN binaries b ON b.id = f.binary_id;''')\n",
    "for info in infos:\n",
    "    csvrows.append([x for x in info]) \n",
    "\n",
    "df = pd.DataFrame(csvrows, columns=['function_id', 'function_name', 'rva_start', 'binary_id', 'binary_compiler', 'binary_optimization', 'binary_github_url'])\n",
    "df = df[df['function_id'].isin(fids)]\n",
    "df.to_csv(CSVOUT, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort out the binary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block flattens the dataset folder into flatten folders, each binary stay in its own folder, folder name is binary id, along with its pdbs\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import sqlite3\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import hashlib\n",
    "\n",
    "def getmd5(s):\n",
    "    return hashlib.md5(s.encode()).hexdigest()\n",
    "\n",
    "dbfile = 'sept25.sqlite'\n",
    "dataset_path = 'dataset_sept25'\n",
    "flatten_dir = \"dataset\"\n",
    "\n",
    "if os.path.exists(flatten_dir):\n",
    "    os.system(f\"rm -rf {flatten_dir}\")\n",
    "os.makedirs(flatten_dir)\n",
    "\n",
    "connection = sqlite3.connect(dbfile)\n",
    "cursor = connection.cursor()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(CSVOUT)\n",
    "binaryids = df['binary_id'].unique()\n",
    "\n",
    "infos = cursor.execute('SELECT id, path, file_name, optimization, github_url, toolset_version FROM binaries;')\n",
    "for binid, path, file_name, opt, github_url,toolset_version in tqdm(infos):\n",
    "    full_path = os.path.join(dataset_path, path.replace(\"\\\\\", \"/\"))\n",
    "    if int(binid) not in binaryids:\n",
    "        continue\n",
    "    if not os.path.isfile(full_path):\n",
    "        print(\"Missing\", full_path)\n",
    "        continue\n",
    "    if not os.path.isdir(os.path.join(flatten_dir, str(binid))):\n",
    "        os.makedirs(os.path.join(flatten_dir, str(binid)))\n",
    "    # Original datautils/dataset/libcap-git-setcap-O2-8dc43f20ea80b7703f6973a1ea86e8b8\n",
    "    shutil.copy(full_path, os.path.join(flatten_dir, str(binid), f\"{binid}_{file_name}-{toolset_version}-{opt}-{getmd5(github_url)}\"))\n",
    "    newcursor = connection.cursor()\n",
    "    pdbs = newcursor.execute('SELECT pdb_path FROM pdbs where binary_id = ?', (binid,))\n",
    "    for pdb in pdbs:\n",
    "        full_path = os.path.join(dataset_path, pdb[0].replace(\"\\\\\", \"/\"))\n",
    "        if not os.path.isfile(full_path):\n",
    "            print(\"Missing\", full_path)\n",
    "            continue\n",
    "        shutil.copy(full_path, os.path.join(flatten_dir, str(binid), os.path.basename(pdb[0].replace(\"\\\\\", \"/\"))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The jTrans code from origial author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jTrans code, not modified much\n",
    "import os\n",
    "import subprocess\n",
    "import multiprocessing\n",
    "import time\n",
    "from util.pairdata import pairdata\n",
    "from subprocess import STDOUT, check_output\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "ida_path=\"idat64\"\n",
    "script_path = \"./process.py\"\n",
    "\n",
    "\n",
    "os.system(\"rm -rf extract&&mkdir extract\")\n",
    "os.system(\"rm -rf log&&mkdir log\")\n",
    "os.system(\"rm -rf idb&&mkdir idb\")\n",
    "\n",
    "def getTarget(path, prefixfilter=None):\n",
    "    return [x for x in glob.glob(f'{path}/**/*', recursive=True) if os.path.isfile(x) and (prefixfilter is None or any([x.startswith(y) for y in prefixfilter]))]\n",
    "\n",
    "def cmd_warp(cmd, timeout):\n",
    "    output = check_output(cmd, stderr=STDOUT, timeout=timeout)\n",
    "    return\n",
    "\n",
    "dataset_dir = \"dataset\"\n",
    "start = time.time()\n",
    "target_list = getTarget(dataset_dir)\n",
    "\n",
    "pool = multiprocessing.Pool(processes=128)\n",
    "for target in target_list:\n",
    "    if target.lower().endswith(\"lib\") or target.lower().endswith(\"pdb\"):\n",
    "        continue\n",
    "    filename = os.path.basename(target)\n",
    "    filename_strip = filename\n",
    "    cmd = [ida_path, f'-Llog/{filename}.log', '-c', '-A', f'-S{script_path}', f'-oidb/{filename}.idb', f'{target}']\n",
    "    pool.apply_async(cmd_warp, args=(cmd, 600, ))\n",
    "\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "pairdata(\"extract\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code block will create a file binid2hash2id.json, and a folder addr_ref for performance issue. \n",
    "If we query sqlite all the time it's super slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import hashlib\n",
    "import sys\n",
    "import os\n",
    "import sqlite3\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import hashlib\n",
    "import pickle\n",
    "from hashlib import sha256\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "def getmd5(s):\n",
    "    return hashlib.md5(s.encode()).hexdigest()\n",
    "\n",
    "def sha256sum(b):\n",
    "    h1 = sha256()\n",
    "    h1.update(b)\n",
    "    return h1.digest().hex()\n",
    "\n",
    "dbfile = 'sept25.sqlite'\n",
    "\n",
    "connection = sqlite3.connect(dbfile)\n",
    "cursor = connection.cursor()\n",
    "\n",
    "\n",
    "db = {}\n",
    "\n",
    "for function_name, binid, rva in tqdm(cursor.execute(f'SELECT f.name, f.binary_id, r.start FROM functions f JOIN rvas r ON f.id==r.function_id;')):\n",
    "    if binid not in db:\n",
    "        db[binid] = {}\n",
    "    db[binid][function_name] = rva\n",
    "    \n",
    "os.system(f\"rm -rf addr_ref\")\n",
    "os.system(f\"mkdir -p addr_ref\")\n",
    "for binid in tqdm(db):\n",
    "    with open(f'./addr_ref/{binid}.json', 'w') as f:\n",
    "        json.dump(db[binid], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Addrress convert codes, it reads extract and convert addresses to extracted_modify  \n",
    "Convert jTrans info {sub_xxxx:[addr, insts, bytes, cfg, bai_feture]} to {real_name:[addr, insts, bytes, cfg, bai_feture]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import hashlib\n",
    "import sys\n",
    "import os\n",
    "import sqlite3\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import hashlib\n",
    "import pickle\n",
    "from hashlib import sha256\n",
    "import pefile\n",
    "import multiprocessing\n",
    "\n",
    "def getmd5(s):\n",
    "    return hashlib.md5(s.encode()).hexdigest()\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "def sha256sum(b):\n",
    "    h1 = sha256()\n",
    "    h1.update(b)\n",
    "    return h1.digest().hex()\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def run(f):\n",
    "    binid = os.path.basename(f).split(\"_\")[0]\n",
    "    with open(f'addr_ref/{binid}.json', 'r') as fh:\n",
    "        name2addr = json.load(fh)\n",
    "    addr2name = {name2addr[x]:x for x in name2addr}\n",
    "    # sort by address\n",
    "    addr2name = {k: v for k, v in sorted(addr2name.items(), key=lambda item: item[0], reverse=True)}\n",
    "    fpath = os.path.join(\"dataset\", binid, os.path.basename(f).split('_extract.pkl')[0])\n",
    "\n",
    "    with open(f, \"rb\") as fh:\n",
    "        saved_index = pickle.load(fh)\n",
    "    keys_stored = list(saved_index.keys())\n",
    "    for x in keys_stored:\n",
    "        addr = saved_index[x][0]\n",
    "        peobj = pefile.PE(fpath, fast_load=True)\n",
    "        relative_addr = saved_index[x][0] - peobj.OPTIONAL_HEADER.ImageBase\n",
    "        if relative_addr in addr2name:\n",
    "            name = addr2name[relative_addr]\n",
    "        else:\n",
    "            for addr, name in addr2name.items():\n",
    "                if addr <= relative_addr:\n",
    "                    name = addr2name[addr]\n",
    "                    break\n",
    "        saved_index[name] = saved_index.pop(x)\n",
    "    with open(os.path.join(\"./extracted_modify\", os.path.basename(f)), \"wb\") as fh:\n",
    "        pickle.dump(saved_index, fh)\n",
    "        \n",
    "\n",
    "pool = multiprocessing.Pool(processes=128)\n",
    "\n",
    "for f in tqdm(glob.glob(\"extract/**/*\", recursive=True)):\n",
    "    if f.endswith(\"extract.pkl\"):\n",
    "        pool.apply_async(run, args=(f,))\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output to extract_selected folder, which will be used to eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import hashlib\n",
    "import sys\n",
    "import os\n",
    "import sqlite3\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import hashlib\n",
    "import pickle\n",
    "from hashlib import sha256\n",
    "import pefile\n",
    "import multiprocessing\n",
    "\n",
    "def getmd5(s):\n",
    "    return hashlib.md5(s.encode()).hexdigest()\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "def sha256sum(b):\n",
    "    h1 = sha256()\n",
    "    h1.update(b)\n",
    "    return h1.digest().hex()\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "        \n",
    "df = pd.read_csv(CSVOUT)\n",
    "print(\"Read csv file\")\n",
    "# Calculate the hash of each function\n",
    "for f in tqdm(glob.glob(\"extracted_modify/**/*\", recursive=True)):\n",
    "    if f.endswith(\"extract.pkl\"):\n",
    "        binid = os.path.basename(f).split(\"_\")[0]\n",
    "        df_selected = df[df['binary_id'] == int(binid)]\n",
    "        new_index = {}\n",
    "        with open(f, \"rb\") as fh:\n",
    "            saved_index = pickle.load(fh)\n",
    "        for func_name in saved_index:\n",
    "            if func_name in df_selected['function_name'].values:\n",
    "                new_index[func_name] = saved_index[func_name]\n",
    "        # print(len(new_index), len(df_selected['function_name'].values)) \n",
    "        with open(os.path.join(\"./extract_selected\", os.path.basename(f)), \"wb\") as fh:\n",
    "            pickle.dump(new_index, fh)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jtrans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
